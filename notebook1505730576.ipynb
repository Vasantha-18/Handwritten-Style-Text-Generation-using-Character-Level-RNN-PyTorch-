{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4970401,"sourceType":"datasetVersion","datasetId":2882872}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Import Libraries\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n# Step 2: Sample Text Data (can be replaced with any large corpus)\ntext = \"hello world handwritten text generation using rnn model in pytorch\"\nchars = sorted(list(set(text)))\nchar_to_idx = {ch: i for i, ch in enumerate(chars)}\nidx_to_char = {i: ch for i, ch in enumerate(chars)}\nn_chars = len(chars)\n\n# Step 3: One-Hot Encoding Function\ndef one_hot_encode(sequence, n_chars):\n    data = np.zeros((len(sequence), n_chars))\n    for i, char in enumerate(sequence):\n        data[i, char_to_idx[char]] = 1\n    return data\n\n# Step 4: Create Input and Target Sequences\nseq_length = 10\ninput_data = []\ntarget_data = []\n\nfor i in range(0, len(text) - seq_length):\n    input_seq = text[i:i+seq_length]\n    target_char = text[i+seq_length]\n    input_data.append(one_hot_encode(input_seq, n_chars))\n    target_data.append(char_to_idx[target_char])\n\n# ⚠️ Performance Fix: Convert list of arrays to a single NumPy array first\ninput_array = np.array(input_data)\nX = torch.tensor(input_array, dtype=torch.float32)       # shape: (samples, seq_length, n_chars)\ny = torch.tensor(target_data, dtype=torch.long)          # shape: (samples)\n\n# Step 5: Define the RNN Model\nclass CharRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n        super(CharRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n\n        self.rnn = nn.LSTM(input_size, hidden_size, n_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, hidden):\n        out, hidden = self.rnn(x, hidden)\n        out = self.fc(out[:, -1, :])  # Take only the last output\n        return out, hidden\n\n    def init_hidden(self, batch_size):\n        return (torch.zeros(self.n_layers, batch_size, self.hidden_size),\n                torch.zeros(self.n_layers, batch_size, self.hidden_size))\n\n# Step 6: Initialize Model, Loss Function, and Optimizer\nmodel = CharRNN(input_size=n_chars, hidden_size=128, output_size=n_chars, n_layers=1)\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n\n# Step 7: Training the Model\nn_epochs = 100\nfor epoch in range(n_epochs):\n    hidden = model.init_hidden(X.size(0))\n    optimizer.zero_grad()\n    output, hidden = model(X, hidden)\n    loss = loss_fn(output, y)\n    loss.backward()\n    optimizer.step()\n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.4f}\")\n\n# Step 8: Generate Text Function\ndef generate_text(model, start_str, gen_length):\n    model.eval()\n    result = start_str\n    hidden = model.init_hidden(1)\n\n    # Prepare initial input\n    input_seq = one_hot_encode(start_str[-seq_length:], n_chars)\n    input_seq = torch.tensor(input_seq, dtype=torch.float32).unsqueeze(0)\n\n    for _ in range(gen_length):\n        output, hidden = model(input_seq, hidden)\n        predicted_idx = torch.argmax(output, dim=1).item()\n        predicted_char = idx_to_char[predicted_idx]\n        result += predicted_char\n\n        # Prepare next input\n        next_input = one_hot_encode(result[-seq_length:], n_chars)\n        input_seq = torch.tensor(next_input, dtype=torch.float32).unsqueeze(0)\n\n    return result\n\n# Step 9: Test the Generator\ngenerated = generate_text(model, start_str=\"hello wor\", gen_length=100)\nprint(\"\\nGenerated Text:\\n\" + generated)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-04T14:24:19.045505Z","iopub.execute_input":"2025-06-04T14:24:19.045726Z","iopub.status.idle":"2025-06-04T14:24:28.704908Z","shell.execute_reply.started":"2025-06-04T14:24:19.045704Z","shell.execute_reply":"2025-06-04T14:24:28.704001Z"}},"outputs":[{"name":"stdout","text":"Epoch 10/100, Loss: 2.7298\nEpoch 20/100, Loss: 2.5849\nEpoch 30/100, Loss: 2.2432\nEpoch 40/100, Loss: 1.2910\nEpoch 50/100, Loss: 0.3317\nEpoch 60/100, Loss: 0.0509\nEpoch 70/100, Loss: 0.0134\nEpoch 80/100, Loss: 0.0061\nEpoch 90/100, Loss: 0.0038\nEpoch 100/100, Loss: 0.0028\n\nGenerated Text:\nhello wordd d nndit nttexxgmon gmod d yrceetxxgmletexggmd prhd pode  yornd ai ptchnnohin toxngmhinnpohinpochi\n","output_type":"stream"}],"execution_count":1}]}